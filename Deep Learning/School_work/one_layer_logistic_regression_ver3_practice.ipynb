{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c098d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d2fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './datasets/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# 1*28*28 -> 784 = 28*28\n",
    "train_data = MNIST(root=path,train=True,transform=transform,download=True)\n",
    "test_data = MNIST(root=path,train=False,transform=transform,download=True)\n",
    "\n",
    "# choose train data with label 0 or 1\n",
    "idx = (train_data.targets==0) | (train_data.targets==1)\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "\n",
    "# choose test data with label 0 or 1\n",
    "idx = (test_data.targets==0) | (test_data.targets==1)\n",
    "test_data.targets = test_data.targets[idx]\n",
    "test_data.data = test_data.data[idx]\n",
    "\n",
    "batch_size = 85\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data,batch_size=len(test_data),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11002ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HERE ####\n",
    "# we are assuming one layer logistic regression\n",
    "w = np.random.randn(784,1)\n",
    "b = np.random.randn(1,1)\n",
    "eta = 1e-4 # learning rate\n",
    "delta = 1e-10 # prevent log 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4b0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function\n",
    "def sigmoid(val):\n",
    "    result=1/(1+np.exp(-val))\n",
    "    return result\n",
    "\n",
    "#### HERE ####\n",
    "# define derivative of sigmoid function w.r.t. its value\n",
    "def grad_sigmoid(val):\n",
    "    return sigmoid(val)*(1-sigmoid(val))\n",
    "\n",
    "# given data instances in batch form,\n",
    "# compute loss and gradients of w and b\n",
    "# also, count the number of correct prediction\n",
    "def compute_loss_and_grad(data_instance):\n",
    "    x, y = data_instance\n",
    "    ## forward pass\n",
    "    linear=np.matmul(x,w)+b\n",
    "    y_est=sigmoid(linear)\n",
    "    loss=-y*np.log(y_est+delta)-(1-y)*np.log(1-y_est+delta)\n",
    "    \n",
    "    ## grad computation\n",
    "    grad=-y*(1-sigmoid(linear))+(1-y)*sigmoid(linear)\n",
    "    grad_w=np.multiply(grad,w)\n",
    "    grad_b=grad\n",
    "    hit=(y==np.round(y_est))\n",
    "    \n",
    "    return loss,(grad_w,grad_b),hit ## 85ê°œ size (==batch_size)\n",
    "\n",
    "\n",
    "# update NN parameters w and b with SGD\n",
    "def update_parameters(params,grads):\n",
    "    w, b = params\n",
    "    grad_w, grad_b = grads\n",
    "    # fill out here and return the variables correctly anyway you want to    \n",
    "    ##############\n",
    "    #### HERE ####\n",
    "    w-=eta*np.mean(grad_w,axis=0).reshape(-1,1)\n",
    "    b-=eta*np,mean(grad_b,axis=0).reshape(-1,1)\n",
    "    ##############\n",
    "   \n",
    "    return w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591e77c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epoch = 500\n",
    "\n",
    "for i in range(num_epoch):\n",
    "\n",
    "    # train the logistic regression model\n",
    "    total_loss_train = 0\n",
    "    count = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # 85*1*28*28 -> -1=85,784\n",
    "        # compute loss and gradients, and then update the parameters\n",
    "        # also, compute sum of the loss and the number of correct prediction in the batch\n",
    "        x, y = x.numpy().reshape(-1,784), y.numpy().reshape(-1,1)\n",
    "        params = (w, b)\n",
    "        loss,grad,hit=compute_loss_and_grad((x,y))\n",
    "        w,b=update_parameters(params,grads)\n",
    "        total_loss_train+=loss.sum()\n",
    "        count+=hit.sum()\n",
    "    \n",
    "    #### HERE ####\n",
    "    # compute average loss and accuracy for the train dataset\n",
    "    loss_train = total_loss_train/len(train_data)\n",
    "    acc_train = count/len(train_data)\n",
    "    \n",
    "    # test, or evaluate, the trained logistic regression model\n",
    "    dataiter = iter(test_loader)\n",
    "    te_images, te_labels = dataiter.next()\n",
    "    \n",
    "    te_images,te_labels=te_images.numpy().reshape(-1,784),te_labels.numpy().reshape(-1,1)\n",
    "    \n",
    "    loss,-,hit=compute_loss_and_grad((te_images,te_labels))\n",
    "    total_loss_test=loss.sum()\n",
    "    loss_test=total_loss_test/len(test_data)\n",
    "    acc_test=hit.sum()/len(test_data)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Epoch %d Train: %.3f / %.2f %%\"%(i,loss_train,acc_train*100))\n",
    "        print(\"Epoch %d Test: %.3f / %.2f %%\"%(i,loss_test,acc_test*100))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3272e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
